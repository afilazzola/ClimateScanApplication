library(tidyverse)
library(wordcloud)
invalid 'cex' value
## subset based on departemtn
dept.words <- subset(df.in, department==input$department)
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
df.in
str(df.in)
df.in[,"Future.risk.quant"] <- df.in[,"sum.future.quant"]
df.in[,"department"] <- paste(df.in[,"Department"],df.in[,"Division"], sep=":") ## creates a unique column that combines division and subidentifier
str(df.in)
dept.words <- subset(df.in, department=="CORP:Recreation & Culture")
dept.words
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
stop3
data.frame(stop3)
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
?wordcloud
dept.words <- subset(df.in, department==df.in$department[1])
dept.words <- subset(df.in, department==df.in$department[1])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==df.in$department[2])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==df.in$department[3])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
length(df.in$department)
unique(df.in$department)
is.na(data.frame(stop3))
sum(is.na(data.frame(stop3)))
stop3[1,1] <- "NAN"
sum(is.na(data.frame(stop3)))
stop3[1,1] <- "NaN"
sum(is.na(data.frame(stop3)))
stop3[1,1] <- "NA"
sum(is.na(data.frame(stop3)))
stop3
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[1])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[2])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[3])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[4])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
dept.words <- subset(df.in, department==unique(df.in$department)[5])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[6])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[7])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[8])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[9])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[10])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[11])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[12])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[13])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[15])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[16])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[17])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[18])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[19])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=1)
data.frame(stop3)
warnings()
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=2)
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=2)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[20])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=2)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[21])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=2)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[22])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=2)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[23])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=2)
data.frame(stop3)
dept.words <- subset(df.in, department==unique(df.in$department)[24])
## load text from Then and So
text_df <- data_frame(text=as.character(dept.words$Then),as.character(dept.words$So))
word.count <- text_df %>% unnest_tokens(word,text) %>%  dplyr::count(word, sort=T) ## count words
## list stopwords
stopwords <- c(stop_words$word, "risk","e.g.","e.g","due","increases","decreases","impacts","impact","lack","risks", "increase","increased","increasing","decrease","decreased","decreasing","lower","low","high","higher")
## remove stopwords
stop1 <- text_df %>% unnest_tokens(word,text) ## separate words into separate lines
stop2 <- stop1[!stop1$word %in% stopwords,] ## remove stop words
stop3 <- stop2 %>% dplyr::count(word, sort=T) ## sort words by frequency
wordcloud(stop3$word, stop3$n, max.words=60, min.freq=2)
data.frame(stop3)
unique(df.in$department)[24]
runApp()
runApp()
runApp()
df.in
df.in$department
unique(df.in$department)
class(unique(df.in$department))
department <- as.vector(unique(df.in$department))
department
class(department)
runApp()
runApp()
runApp()
df.in
dept.options <- as.vector(unique(df.in$department))
dept.options
runApp()
runApp()
runApp()
runApp()
runApp()
library(rsconnect)
deployApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?downloadHandler
foxie
runApp()
library(rsconnect)
deployApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
filedata
runApp()
runApp()
runApp()
runApp()
?downloadhander
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
?downloadButton
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
?downloadHandler
runApp()
runApp()
runApp()
runApp()
paste0("ClimScan", Sys.Date(), ".csv", sep="")
runApp()
shiny::runApp()
downloadHandler
runApp()
runApp()
runApp()
library(rsconnect)
deployApp()
runApp()
devtools::install_github('hadley/ggplot2')
library(rsconnect)
deployApp()
options(shiny.fullstacktrace = TRUE)
runApp()
runApp()
runApp()
deployApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
deployApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
